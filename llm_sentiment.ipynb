{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":93282,"databundleVersionId":11098970,"sourceType":"competition"},{"sourceId":104449,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":68809,"modelId":91102}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:12:55.340976Z","iopub.execute_input":"2025-02-17T18:12:55.341274Z","iopub.status.idle":"2025-02-17T18:12:57.206732Z","shell.execute_reply.started":"2025-02-17T18:12:55.341254Z","shell.execute_reply":"2025-02-17T18:12:57.205740Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Setting Up PyTorch and Dependencies","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install pip3-autoremove\n!pip-autoremove torch torchvision torchaudio -y\n!pip install torch xformers --index-url https://download.pytorch.org/whl/cu121\n!pip install unsloth\n!pip install --no-deps trl peft accelerate bitsandbytes datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:13:05.075344Z","iopub.execute_input":"2025-02-17T18:13:05.075752Z","iopub.status.idle":"2025-02-17T18:17:24.225479Z","shell.execute_reply.started":"2025-02-17T18:13:05.075728Z","shell.execute_reply":"2025-02-17T18:17:24.224511Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Importing Libraries for LLM Training and Data Processing","metadata":{}},{"cell_type":"code","source":"# Data Processing and Visualization\nimport os\nimport numpy as np\nimport pandas as pd\n\n# Libraries for Training LLMs\nimport torch\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments, TextStreamer\nfrom unsloth import FastLanguageModel, is_bfloat16_supported\nfrom datasets import Dataset, load_dataset, DatasetDict\n\n# Model Saving\nfrom transformers import AutoModelForSequenceClassification\n\n# LLM Model and Tokenizer Utilities\nfrom transformers import (AutoModelForCausalLM, \n                          DataCollatorForLanguageModeling,\n                          AutoTokenizer, \n                          BitsAndBytesConfig, \n                          TrainingArguments,\n                          Trainer,\n                          DataCollatorForSeq2Seq,\n                          logging)\n\n# Evaluation Metrics\nfrom sklearn.metrics import (accuracy_score, \n                             classification_report, \n                             confusion_matrix)\nfrom sklearn.model_selection import train_test_split\n\n# Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:17:44.259233Z","iopub.execute_input":"2025-02-17T18:17:44.259545Z","iopub.status.idle":"2025-02-17T18:18:20.026061Z","shell.execute_reply.started":"2025-02-17T18:17:44.259509Z","shell.execute_reply":"2025-02-17T18:18:20.025150Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading and Configuring the LLaMA 3 Model","metadata":{}},{"cell_type":"code","source":"# Define the base model path  \nbase_model_name = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2\"\n\n# Set model configuration parameters  \nmax_seq_length = 2048\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True\n\n# Load the pre-trained model and tokenizer  \nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = base_model_name,\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)\n\n# Apply LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning  \nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0,\n    bias = \"none\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n    use_rslora=True,\n    use_gradient_checkpointing=\"unsloth\",\n    loftq_config = None,\n    random_state = 3407\n)\n\nprint(model.print_trainable_parameters())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:18:50.233068Z","iopub.execute_input":"2025-02-17T18:18:50.233452Z","iopub.status.idle":"2025-02-17T18:20:28.954664Z","shell.execute_reply.started":"2025-02-17T18:18:50.233424Z","shell.execute_reply":"2025-02-17T18:20:28.953903Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Formatting Text for Sentiment Classification Input-Output Pairs","metadata":{}},{"cell_type":"code","source":"# Define the prompt template for classification  \ndata_prompt = \"\"\"Classify the text into 'Positive', 'Negative', and return the answer as the predicted sentiment.\n### Input:\n{}\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token\n\ndef formatting_prompt(examples):\n    inputs       = examples[\"sentence\"]\n    outputs      = examples[\"label\"]\n    texts = []\n    for input_, output in zip(inputs, outputs):\n        text = data_prompt.format(input_, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\npass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:20:53.186923Z","iopub.execute_input":"2025-02-17T18:20:53.187265Z","iopub.status.idle":"2025-02-17T18:20:53.191921Z","shell.execute_reply.started":"2025-02-17T18:20:53.187236Z","shell.execute_reply":"2025-02-17T18:20:53.191149Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load training and testing datasets","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/multi-lingual-sentiment-analysis/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/multi-lingual-sentiment-analysis/test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:20:58.115095Z","iopub.execute_input":"2025-02-17T18:20:58.115386Z","iopub.status.idle":"2025-02-17T18:20:58.188296Z","shell.execute_reply.started":"2025-02-17T18:20:58.115365Z","shell.execute_reply":"2025-02-17T18:20:58.187621Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Converting DataFrames to Datasets and Applying Preprocessing","metadata":{}},{"cell_type":"code","source":"training_data = Dataset.from_pandas(train_df)\ntesting_data = Dataset.from_pandas(test_df)\ntraining_data = training_data.map(formatting_prompt, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:22:05.178300Z","iopub.execute_input":"2025-02-17T18:22:05.178600Z","iopub.status.idle":"2025-02-17T18:22:05.327949Z","shell.execute_reply.started":"2025-02-17T18:22:05.178579Z","shell.execute_reply":"2025-02-17T18:22:05.327013Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Configuring Hyperparameters for Model Training","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer, SFTConfig\nfrom transformers import TrainingArguments\n\nsft_config = SFTConfig(\n    learning_rate = 3e-4,\n    dataset_text_field=\"text\",  \n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=True,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    num_train_epochs=20,\n    fp16=not is_bfloat16_supported(),\n    bf16=is_bfloat16_supported(),\n    logging_steps=1,\n    optim=\"adamw_8bit\",\n    lr_scheduler_type = \"linear\",\n    weight_decay=0.01,\n    warmup_steps=5,\n    max_steps=70,\n    output_dir=\"output\",\n    seed=3407,\n    report_to=\"none\",\n)\n\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=training_data,\n    args=sft_config,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:23:54.145373Z","iopub.execute_input":"2025-02-17T18:23:54.145719Z","iopub.status.idle":"2025-02-17T18:23:58.907245Z","shell.execute_reply.started":"2025-02-17T18:23:54.145695Z","shell.execute_reply":"2025-02-17T18:23:58.906292Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:27:36.621403Z","iopub.execute_input":"2025-02-17T18:27:36.621796Z","iopub.status.idle":"2025-02-17T19:29:11.227408Z","shell.execute_reply.started":"2025-02-17T18:27:36.621753Z","shell.execute_reply":"2025-02-17T19:29:11.226663Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = FastLanguageModel.for_inference(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T19:29:57.813507Z","iopub.execute_input":"2025-02-17T19:29:57.813980Z","iopub.status.idle":"2025-02-17T19:29:57.818183Z","shell.execute_reply.started":"2025-02-17T19:29:57.813946Z","shell.execute_reply":"2025-02-17T19:29:57.817280Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test = testing_data.to_pandas()\nX_train = training_data.to_pandas()\nunique_labels = X_train['label'].unique()\nprint(unique_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T19:30:01.619649Z","iopub.execute_input":"2025-02-17T19:30:01.619986Z","iopub.status.idle":"2025-02-17T19:30:01.668421Z","shell.execute_reply.started":"2025-02-17T19:30:01.619958Z","shell.execute_reply":"2025-02-17T19:30:01.667695Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Making Predictions with the Model on Test Data","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef predict(test, model, tokenizer):\n    global unique_labels\n    y_pred = []\n    categories = unique_labels\n    # model.config.pad_token_id = model.config.eos_token_id\n    \n    for i in tqdm(range(len(test))):\n        sent = test.iloc[i][\"sentence\"]\n\n        # Tokenize input\n        inputs = tokenizer(\n            [data_prompt.format(sent,\"\",)], return_tensors=\"pt\").to(\"cuda\")\n\n        # Generate output from the model\n        with torch.no_grad():\n            outputs = model.generate(**inputs, max_new_tokens=10, temperature=0.1)\n\n        # Decode output\n        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n        # Extract label from generated text\n        answer = answer.split(\"### Response:\")[-1].strip()\n\n        # Determine the predicted category\n        for category in categories:\n            if category.lower() in answer.lower():\n                y_pred.append(category)\n                break\n        else:\n            y_pred.append(\"Positive\")\n    \n    return y_pred\n\ny_pred = predict(X_test, model, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T19:30:04.050495Z","iopub.execute_input":"2025-02-17T19:30:04.050816Z","iopub.status.idle":"2025-02-17T19:30:51.774139Z","shell.execute_reply.started":"2025-02-17T19:30:04.050773Z","shell.execute_reply":"2025-02-17T19:30:51.773362Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.DataFrame({\n    \"ID\": range(1,101),\n    \"label\": y_pred\n})\n\nsubmission.to_csv(\"submission.csv\", index = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T19:31:19.336874Z","iopub.execute_input":"2025-02-17T19:31:19.337179Z","iopub.status.idle":"2025-02-17T19:31:19.368685Z","shell.execute_reply.started":"2025-02-17T19:31:19.337158Z","shell.execute_reply":"2025-02-17T19:31:19.368037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}